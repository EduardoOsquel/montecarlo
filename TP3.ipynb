{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOt3WQ8G0qgQr//2g5H2/CD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Modelling for BME - Inverse problem\n","### Practice 3\n","### Student: Eduardo Osquel Perez Rivero"],"metadata":{"id":"94y5jTSKUrLV"}},{"cell_type":"markdown","source":["First, we prepare all the necessary libraries"],"metadata":{"id":"TOsGnWlCVAOT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmLjMISZTgIS"},"outputs":[],"source":["# Install necessary packages\n","!pip install numpy pmcx jdata bjdata matplotlib"]},{"cell_type":"markdown","source":["Then, we import them into our notebook to start working with them"],"metadata":{"id":"gxPnet3MVPuE"}},{"cell_type":"code","source":["# Import libraries\n","import numpy as np\n","import pmcx\n","import jdata as jd\n","import matplotlib.pyplot as plt\n","import h5py\n","from scipy.sparse import lil_matrix, linalg, diags\n","from scipy.linalg import pinv"],"metadata":{"id":"Rao71muSTlJx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next step is to check that we have a GPU to be able to use the tools offered by MonteCarlo"],"metadata":{"id":"_EqTl_MzWKn5"}},{"cell_type":"code","source":["# Print pmcx version\n","print(\"pmcx version:\", pmcx.__version__)\n","\n","# Check GPU availability\n","pmcx.gpuinfo()"],"metadata":{"id":"Sm-4yXuyToIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We first read the data from MATLAB provided dataframes"],"metadata":{"id":"yYwhX8gJWezz"}},{"cell_type":"code","source":["# Load data from 'A.mat'\n","with h5py.File('A.mat', 'r') as f:\n","    arrays_A = {k: np.array(v) for k, v in f.items()}\n","A = arrays_A['A']\n","\n","# Load data from 'y_mes.mat'\n","with h5py.File(\"y_mes.mat\", 'r') as f:\n","    arrays_y_mes = {k: np.array(v) for k, v in f.items()}\n","y_mes = arrays_y_mes['y_mes']"],"metadata":{"id":"0xCZCk96TpW_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we check the shape and the condition number of matrix A"],"metadata":{"id":"P14hrwuTYO5J"}},{"cell_type":"code","source":["print(f\"The shape of A matrix is {A.shape}\")\n","print(f\"The condition number of A matrix is {np.linalg.cond(A)}\")"],"metadata":{"id":"deRd7b49Ts1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the results above, we can see that the shape of matrix A is 2D, with 15625 rows and 5625 columns. The condition number of A is around 1009987, indicating a very large value. This large condition number suggests that A is closer to being a singular matrix, meaning it is highly ill-conditioned. A high condition number implies that small changes in the input may lead to significant changes in the output. Conversely, a lower condition number indicates that the matrix is relatively stable in response to input changes."],"metadata":{"id":"-UE8DIHhYTAo"}},{"cell_type":"markdown","source":["The next step involves defining vector x to obtain y = Ax. The transpose of y is 5625 * 1, and the transpose of A is 5625 * 15625. Therefore, we define x as a vector with a shape of 15625 * 1. To compare the result of the inverse with the true value of x, we attempt to define the true value of x as a vector where each element is equal to 2."],"metadata":{"id":"OqDv5ievY8hf"}},{"cell_type":"code","source":["x = np.ones((15625*1)) + 1\n","x.shape"],"metadata":{"id":"I-h-gPl0Tw61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we can get y and y_noise"],"metadata":{"id":"677xeBp_ZpVt"}},{"cell_type":"code","source":["y = np.dot(np.transpose(A) ,x)\n","y = y.reshape((5625,1))\n","\n","n = np.random.randn(5625,1)\n","n.shape\n","\n","y_noise = y + n\n","y_noise"],"metadata":{"id":"PAQBdYmNT2Kt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because matrix A is irreversible and singular, we cannot use $$x=A ^{-1} * y$$ to obtain x.<br />"],"metadata":{"id":"c13I2MkraIRv"}},{"cell_type":"code","source":["A.shape"],"metadata":{"id":"vngGdg8rT3yU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hence, we'll employ an alternative method to solve the inverse problem and then compare the outcome with the true value of x. For this purpose, we will solve the inverse problem both with and without the inclusion of noise, which will allow us to observe the MSE anomalies.\n","\n","Without noise:"],"metadata":{"id":"ySnne-LRaYqO"}},{"cell_type":"code","source":["x_without_noise = np.linalg.lstsq(A.transpose() , y , rcond=None)[0]\n","x_without_noise\n","\n","mse = np.mean((x - x_without_noise) ** 2)\n","mse"],"metadata":{"id":"99menElDT7t0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With noise:"],"metadata":{"id":"5jcMwkMCbAzs"}},{"cell_type":"code","source":["x_with_noise = np.linalg.lstsq(A.transpose() , y_noise , rcond=None)[0]\n","x_with_noise\n","\n","mse = np.mean((x - x_with_noise) ** 2)\n","mse"],"metadata":{"id":"RwlGm052T907"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we directly apply the middle least squares method without addressing the noise, we may encounter MSE anomalies. Hence, it's imperative to account for the noise's influence on the results when tackling inverse problems, especially in the presence of noise. Regularization serves as a crucial tool to mitigate the impact of noise, thus enhancing the accuracy of our solutions to the inverse problem"],"metadata":{"id":"ySpyaowsbRic"}},{"cell_type":"markdown","source":["In this case, we employ Tikhonov Regularization (L2) to accomplish this task. Also, after computing the regularization term, we utilize the function pinv to compute the pseudo-inverse of the matrix tikhonov_one"],"metadata":{"id":"uweBl2llbcya"}},{"cell_type":"code","source":["alpha = 0.1  # Set regularization parameter\n","regularization_matrix = alpha * np.eye(A.shape[1])  # Create regularization matrix\n","tikhonov_one = pinv(np.dot(A.T, A) + regularization_matrix)  # Calculate Tikhonov regularization matrix\n","\n","tikhonov_two = np.dot(tikhonov_one, A.T)\n","\n","tikhonov_two.shape = tikhonov_two.shape\n","print(\"Shape of tikhonov_two:\", tikhonov_two.shape)"],"metadata":{"id":"MXn29zlIT_Ci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Subsequently, we obtain tikhonov_two by performing the matrix multiplication tikhonov_one * A.T"],"metadata":{"id":"V-cUlBmucLuW"}},{"cell_type":"code","source":["x_reg = np.dot(tikhonov_two.T, y)\n","\n","mse = np.mean((x - x_reg) ** 2)\n","mse"],"metadata":{"id":"80R6VdbFUE2j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upon incorporating regularization, we observe that the mean squared error (MSE) regression remains consistent, approximately at 72.36%. When contrasted with the scenario devoid of noise, both the computational error and MSE exhibit relatively larger magnitudes owing to the noise influence. In comparison to cases where regularization is not employed for processing, the utilization of ridge regression demonstrates normal MSE regression behavior, indicating enhanced accuracy in solving the inverse problem."],"metadata":{"id":"SwSyZR4QcXqS"}},{"cell_type":"markdown","source":["## Exercise 2"],"metadata":{"id":"gMYK2oAAcbrV"}},{"cell_type":"markdown","source":["We will use \"A\" and \"y_mes\" to solve inverse problem:$$y = Ax$$\n","First we use SVD + L1 regularization to solve inverse problem\n","  $$A = USV^{T}$$\n","  so we can get $$x = VS^{-1}U^ty$$"],"metadata":{"id":"j39XyxFScmVY"}},{"cell_type":"code","source":["U, S, V = np.linalg.svd(A, full_matrices=0)  # Perform SVD decomposition\n","alpha = 0.1  # L1 regularization parameter\n","S_inv = np.diag(1 / (S + alpha))  # Calculate inverse singular values\n","tikhonov_one = np.dot(V.T, S_inv)  # Compute tikhonov_one\n","tikhonov_one.shape\n","\n","tikhonov_two = np.dot(tikhonov_one, U.T)\n","tikhonov_two.shape\n","\n","x1 = np.dot(tikhonov_two.T, y_mes.T)\n","x1.shape"],"metadata":{"id":"lhiuWx2PUKyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we add positive constrains to the A matrix"],"metadata":{"id":"Ftkn5Ou5dBKW"}},{"cell_type":"code","source":["pre_A = A\n","for i in range(15625):\n","    for j in range(5625):\n","        if(A[i][j]) < 0:\n","            A[i][j] = 0"],"metadata":{"id":"CPCjcy3HUPNB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Subsequently, we employ Singular Value Decomposition (SVD) along with L1 regularization to revisit the inverse problem"],"metadata":{"id":"-kQszReGdH0A"}},{"cell_type":"code","source":["U, S, V = np.linalg.svd(pre_A, full_matrices=0)  # Perform SVD decomposition\n","alpha = 0.1  # L1 regularization parameter\n","S_inv = np.diag(1 / (S + alpha))  # Calculate inverse singular values\n","tikhonov_one = np.dot(V.T, S_inv)  # Compute tikhonov_one\n","tikhonov_one.shape  # Get the shape of tikhonov_one\n","\n","tikhonov_two = np.dot(tikhonov_one, U.T)\n","tikhonov_two.shape\n","\n","x2 = np.dot(tikhonov_two.T, y_mes.T)\n","x2.shape"],"metadata":{"id":"o0IE4oYBURpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can visualize the degree of approximation between Ax and y_mes by plotting a graph"],"metadata":{"id":"FJcUDZYSdWF0"}},{"cell_type":"code","source":["dt_true = np.dot(A.T, x1).flatten()\n","dt_observed = y_mes.T.flatten()\n","\n","# Plotting the data\n","plt.plot(dt_true, color='blue', marker=\"o\", label=\"dt_true\")\n","plt.plot(dt_observed, color='red', marker=\"o\", label=\"dt_observed\")\n","\n","# Adding labels and legend\n","plt.xlabel('Index')\n","plt.ylabel('Value')\n","plt.legend()\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"fNMaPC9JUYg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The similarity between the calculated x and the measured y_mes in terms of distribution is evident. Subsequently, we proceed to calculate the residuals and conduct further analysis. Upon merging the two figures above, it is apparent that the images almost perfectly coincide. This indicates a strong agreement between the calculated and measured data distributions"],"metadata":{"id":"1kN2hkhRdl1Z"}},{"cell_type":"code","source":["residuals = dt_observed - dt_true\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(range(len(residuals)), residuals, color='b', label='Residuals')\n","plt.axhline(y=0, color='r', linestyle='--', linewidth=1)  # Add a horizontal line representing zero residuals\n","plt.xlabel('Data Point')\n","plt.ylabel('Residual')\n","plt.title('Residual Plot')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"GvXtTd3qUaGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The residual plot size, at 1e-7, signifies a close match between the measured result y_mes and the calculated x, indicating high precision in the results obtained through SVD decomposition and L1 regularization. Before tackling the inverse problem again, we delve into the ill-posed nature of matrix A. If A proves to be invertible, we can directly multiply the inverse of the A matrix on both sides of the equation to obtain the inverse problem solution. However, if A is singular, alternative methods must be employed for solving it. Analyzing the condition number of matrix A provides insights into its pathological nature. A larger condition number indicates a more severe pathological state for the matrix, where even minor perturbations in measurement can significantly affect the final inverse problem solution. Moreover, in the presence of noise, the ill-posedness of matrix A is further exacerbated, leading to reduced accuracy and stability of the results"],"metadata":{"id":"IPYUNSPid01D"}}]}